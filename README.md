# RAG-система для новостей

---

## Быстрый запуск

```bash
docker compose up -d --build
```

## Структура проекта

```
project/
├─ my_app/                    # основной код
│  ├─ __init__.py             # инициализация пакета
│  ├─ app.py                  # запуск FastAPI + Gradio
│  ├─ data_ingestion.py       # загрузка и индексирование новостей
│  ├─ embeddings.py           # модель эмбеддингов
│  ├─ exceptions.py           # пользовательские исключения
│  ├─ file_processor.py       # чтение и запись JSON/CSV
│  ├─ gradio_interface.py     # Gradio UI
│  ├─ llm_client.py           # взаимодействие с Ollama
│  ├─ qdrant_client.py        # обёртка над Qdrant SDK
│  ├─ search_service.py       # пайплайн: анализ → поиск → генерация
│  └─ text_splitter.py        # семантический чанкинг текста
├─ ├─ evaluation/                          # Оценка качества генерации
│  ├─ rater/                              # Оценка качества ответов
│  │  ├─ evaluate.py                      # Запуск оценщика на Gemini
│  │  ├─ rater.py                       # Код оценщика
│  ├─ cosine_similarity_analysis.py     # Анализ косинусного сходства
│  ├─ input_put_data.csv                # Эталонные ответы (Perfect Answers)
│  ├─ rosberta_1_with_similarity.csv    # Результаты модели 1 с метрикой
│  └─ rosberta_2_with_similarity.csv    # Результаты модели 2 с метрикой
├─ parser/rbc_parser.py       # Парсер новостей
├─ news_db/news_data.json     # JSON с примерами новостей
├─ tests/run_csv_test.py      # оффлайн тестирование по CSV
├─ docker-compose.yml         # конфигурация контейнеров
├─ Dockerfile                 # сборка rag_app
└─ pyproject.toml             # зависимости и конфигурация poetry
```

---

## Как работает система (техническое описание)

### 1. Общая идея

Система реализует подход Retrieval-Augmented Generation (RAG) для обработки русскоязычных текстов новостей. Исходными данными являются JSON-файлы новостей, содержащие тексты, заголовки, источники и временные метки публикаций. Вся эта информация учитывается при обработке и генерации ответов.

Система индексирует данные в векторной базе Qdrant и формирует ответы на вопросы пользователей, используя LLM.

---

### 2. Основные техники и алгоритмы

#### 2.1 Семантический чанкинг

Документы разбиваются на семантически связанные фрагменты (чанки). Система использует модель  LLM для определения оптимальных границ чанков, учитывая смысл и контекст.

#### 2.2 Векторизация и векторный поиск

Каждый чанк преобразуется в векторное представление. Эмбеддинги хранятся и используются в Qdrant для быстрого и точного поиска релевантных фрагментов по запросу пользователя.

#### 2.3 Переформулировка вопросов и извлечение дат

Вопросы пользователей предварительно анализируются и переформулируются моделью. Также модель пытается извлечь из вопросов временные рамки, которые затем используются при фильтрации документов в процессе поиска.

#### 2.4 Интеллектуальный выбор контекста

После векторного поиска модель LLM самостоятельно определяет наиболее релевантный чанк или группу чанков. Затем выбранные чанки объединяются, и модель получает исходный вопрос и полный текст статьи, к которой относится выбранный чанк, для генерации ответа.

#### 2.5 Генерация ответа

Модель LLM генерирует ответ пользователю, используя полный текст выбранного документа. Если релевантного ответа нет, возвращается сообщение о невозможности ответить.

---

### 3. Проверки и валидация

#### 3.1 Проверка вопроса пользователя

Перед обработкой вопроса выполняется проверка на адекватность и отсутствие нецензурных выражений с помощью модели LLM. Если вопрос не проходит проверку, обработка прекращается.

#### 3.2 Валидация сгенерированного ответа

После генерации ответа модель оценивает его релевантность по 5-балльной шкале относительно исходного вопроса. Если ответ получает оценку выше 3, он передаётся пользователю. Иначе возвращается сообщение об отказе в ответе.

# Набор инструментов оценки для RAG‑News

> **Контекст:** данный README описывает **только** папку `evaluation` проекта RAG‑News. Полное описание системы находится в корневом `README.md`.

---

## Оглавление

1. [Назначение](#назначение)
2. [Структура папки](#структура‑папки)
3. [Быстрый старт](#быстрый‑старт)
4. [Пайплайн Cosine‑Similarity](#пайплайн-cosine‑similarity)
5. [Оценка LLM (Gemini)](#оценка-llm-gemini)
6. [Офф‑лайн CSV‑тестирование](#офф‑лайн-csv‑тестирование)
7. [Конфигурация](#конфигурация)
8. [Выходные файлы и отчёты](#выходные‑файлы-и-отчёты)
9. [Интеграция с CI/CD](#интеграция-с-cicd)
10. [Расширение тест‑пака](#расширение-тест‑пака)
11. [FAQ / Траблшутинг](#faq--траблшутинг)
12. [Лицензия](#лицензия)

---

## Назначение<a name="назначение"></a>

Модуль `evaluation` отвечает за измерение качества ответов RAG‑системы двумя комплементарными способами:

| Уровень            | Метрика                                                  | Скрипт                          | Что измеряет                                                     |
| ------------------ | -------------------------------------------------------- | ------------------------------- | ---------------------------------------------------------------- |
| **Автоматическая** | **Косинусное сходство** эмбеддингов                      | `cosine_similarity_analysis.py` | Семантическая близость к эталонному ответу                       |
| **Гибридная**      | **Оценка Gemini 1.5 Pro** (Answer / Chunks / FinalChunk) | `rater/evaluate.py`             | Полнота, фактическая точность, корректность выбранного контекста |

Обе метрики запускаются в CI, и каждую сборку следует считать «зелёной» только при превышении пороговых значений.

---

## Структура папки<a name="структура‑папки"></a>

```text
project/
└─ evaluation/                    # ← текущая директория
   ├─ cosine_similarity_analysis.py   # авто‑метрика
   ├─ input_put_data.csv             # «золотые» ответы
   ├─ rosberta_1_with_similarity.csv # результаты модели 1
   ├─ rosberta_2_with_similarity.csv # результаты модели 2
   │
   ├─ rater/                        # LLM‑оценщик
   │  ├─ evaluate.py                # точка входа
   │  └─ rater.py                   # промпт + рубрика
   │
   └─ README.md                     # (данный файл)
```

---

## Быстрый старт<a name="быстрый‑старт"></a>

### В контейнерах Docker (рекомендуется)

```bash
# собрать и поднять всё в фоне
$ docker compose up -d --build

# (опционально) открыть Gradio‑интерфейс
$ xdg-open http://141.105.64.134:58084/gradio/

# 1) косинусное сходство
$ docker compose exec rag_app \
      python3 /app/evaluation/cosine_similarity_analysis.py

# 2) оценка Gemini (нужен API‑ключ)
$ docker compose exec rag_app \
      python3 /app/evaluation/rater/evaluate.py
```

Скрипты автоматически ищут выходные данные моделей внутри контейнера и сохраняют отчёты в `/app/evaluation/results/`.

### Локальный запуск (без Docker)

1. Установить зависимости:

   ```bash
   poetry install --with evaluation
   ```
2. Задать переменные окружения (см. раздел [Конфигурация](#конфигурация)).
3. Запустить те же команды без префикса `docker compose exec`.

---

## Пайплайн Cosine‑Similarity<a name="пайплайн-cosine‑similarity"></a>

| Шаг   | Действие                        | Детали                                                                                         |
| ----- | ------------------------------- | ---------------------------------------------------------------------------------------------- |
| **1** | Загрузка эталонных и кандидатов | `input_put_data.csv` + `rosberta_*_with_similarity.csv`                                        |
| **2** | Кодирование текстов             | модель `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (можно сменить через CLI) |
| **3** | Расчёт `cos_sim`                | Диапазон `[-1; 1]`. Ближе к 1 ⇒ выше сходство                                                  |
| **4** | Агрегация                       | Среднее, медиана, pass‑rate против *порога* (0.75 по умолчанию)                                |
| **5** | Сохранение                      | CSV + человекочитаемое summary                                                                 |

Пример изменения порога и модели:

```bash
python cosine_similarity_analysis.py \
       --threshold 0.80 \
       --model_name sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens
```

---

## Оценка LLM (Gemini)<a name="оценка-llm-gemini"></a>

### Что измеряется

* **AnswerQuality** — полнота и корректность ответа.
* **ChunksRelevance** — релевантность top‑5 найденных чанков.
* **FinalChunkMatch** — правильность выбора главного чанка.

### Запуск

```bash
export GEMINI_API_KEY="sk-…"
python rater/evaluate.py \
       --model_outputs /app/evaluation/rosberta_1_with_similarity.csv \
       --out_dir /app/evaluation/results/
```

Ключевые параметры:

```
--rubric_path   путь до собственной рубрики (JSON)
--max_calls     ограничить число запросов (для отладки)
```

### Мини‑рубрика по умолчанию

```json
{
  "scales": 10,
  "thresholds": {
    "good": 8,
    "warn": 6
  }
}
```

---

## Офф‑лайн CSV‑тестирование<a name="офф‑лайн-csv‑тестирование"></a>

`tests/run_csv_test.py` прогоняет полный RAG‑конвейер **внутри** контейнера и оценивает ответы на тест‑пак.

```bash
# запустить тест‑пак с настройками по умолчанию
$ docker compose exec rag_app python3 /app/tests/run_csv_test.py

# результаты → /app/tests/results/*.csv
```

Сценарий гарантирует, что:

* каждый ответ получен *≤ 20 с*;
* нет грубых галлюцинаций (по списку стоп‑слов);
* агрегированный CosSim ≥ 0.75 **ИЛИ** Gemini AnswerScore ≥ 8.

---

## Конфигурация<a name="конфигурация"></a>

| Переменная       | Значение по умолчанию                                         | Описание                         |
| ---------------- | ------------------------------------------------------------- | -------------------------------- |
| `GEMINI_API_KEY` | —                                                             | **обязательна** для LLM‑оценщика |
| `CS_MODEL_NAME`  | `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | модель эмбеддингов               |
| `CS_THRESHOLD`   | `0.75`                                                        | порог pass/fail                  |
| `RESULTS_DIR`    | `evaluation/results/`                                         | куда писать отчёты               |

Переменные можно задавать в `.env` или экспортировать в shell; `docker compose` пробрасывает их внутрь контейнера.

---

## Выходные файлы и отчёты<a name="выходные‑файлы-и-отчёты"></a>

После каждого запуска:

```text
evaluation/
└─ results/
   ├─ cos_sim_summary.json      # агрегированные значения
   ├─ cos_sim_detailed.csv      # метрики по каждому вопросу
   ├─ gemini_summary.json       # средние баллы
   └─ gemini_raw_responses.json # полный вывод LLM (для дебага)
```

В CI эти артефакты прикрепляются к билд‑логам GitHub Actions.

---

## Интеграция с CI/CD<a name="интеграция-с-cicd"></a>

* **Pull Request** → GitHub Action `evaluation.yml`:

  1. Разворачивает сервисы через `docker compose`.
  2. Запускает cosine + Gemini‑оценку (Gemini — только на внутренних ветках, чтобы не светить ключ).
  3. Ставит проверку *pass/fail* и прикрепляет артефакты.
* **main** → ночной cron‑джоб прогоняет полный CSV‑тест.

---

## Расширение тест‑пака<a name="расширение-тест‑пака"></a>

1. Добавьте строки в `input_put_data.csv`: 3 колонки — *текст новости*, *вопрос*, *идеальный ответ*.
2. Сгенерируйте ответы модели вручную или через `run_csv_test.py --regen`.
3. Закоммитьте обновлённые CSV и результаты.

> **Совет:** стремитесь к балансированному набору тем: ≥ 10 вопросов на домен (политика, технологии, спорт, …).

---

## FAQ / Траблшутинг<a name="faq--траблшутинг"></a>

| Симптом                       | Возможная причина                       | Решение                                                                        |
| ----------------------------- | --------------------------------------- | ------------------------------------------------------------------------------ |
| `ValueError: cos_sim too low` | модель не нашла новости или база пустая | проверьте, что данные проиндексированы, и заново запустите `data_ingestion.py` |
| `GeminiQuotaExceeded`         | суточный лимит API исчерпан             | используйте платный тариф или запускайте скрипт с флагом `--skip_gemini`       |
| OOM при эмбеддинге            | слишком «тяжёлая» модель                | перейдите на MiniLM‑12L или уменьшите `--batch_size`                           |

---

